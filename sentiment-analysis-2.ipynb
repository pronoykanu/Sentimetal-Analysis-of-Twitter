{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@user when a father is disfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>@user that was fucking weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>@userthat was so shitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams. ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>you are so boring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      1   @user when a father is disfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      1                       @user that was fucking weird\n",
       "3   4      1                          @userthat was so shitty  \n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams. ca...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      1                                  you are so boring"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = df[df['label'] == 0]\n",
    "train_neg = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14784493128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACrlJREFUeJzt3F+InXl9x/H3ZxOiF+pemGlp88cJGNFUCgtDWvCiFpVmFZIbkQSEVhZzFaUo0pSWZU1vWr3wKoUGtC1CTaMXOtiUXNgVStu1mUVdSEJ0SNUMgTra7UIpbUz99mKm9nj2ZOc52ZOczDfvFwTO8zw/znwTJm9+85w5J1WFJKmXx+Y9gCRp9oy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGds7rC+/evbsWFxfn9eUlaVt6/vnnf1RVC1utm1vcFxcXWVlZmdeXl6RtKcn3h6zztowkNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIbm9iam7WLx9N/Me4RWvvfH75v3CNIjwZ27JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDU0KO5JjiS5nmQ1yekJ1/cneTbJN5O8kOS9sx9VkjTUlnFPsgM4CzwJHAJOJDk0tuwPgQtV9QRwHPjTWQ8qSRpuyM79MLBaVTeq6jZwHjg2tqaAN2w+fhy4NbsRJUnTGhL3PcDNkeO1zXOjngE+mGQNuAh8ZNITJTmZZCXJyvr6+j2MK0kaYkjcM+FcjR2fAP6iqvYC7wU+n+Rlz11V56pqqaqWFhYWpp9WkjTIkLivAftGjvfy8tsuTwEXAKrqn4DXArtnMaAkaXpD4n4ZOJjkQJJdbLxgujy25gfAuwCSvI2NuHvfRZLmZMu4V9Ud4BRwCbjGxm/FXElyJsnRzWUfBz6c5NvAF4DfqarxWzeSpAdk55BFVXWRjRdKR889PfL4KvCO2Y4mSbpXvkNVkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhgbFPcmRJNeTrCY5fZc1H0hyNcmVJH812zElSdPYudWCJDuAs8B7gDXgcpLlqro6suYg8PvAO6rqxSS/cL8GliRtbcjO/TCwWlU3quo2cB44Nrbmw8DZqnoRoKp+ONsxJUnTGBL3PcDNkeO1zXOj3gK8Jck/JHkuyZFZDShJmt6Wt2WATDhXE57nIPBOYC/w90neXlX//nNPlJwETgLs379/6mElScMM2bmvAftGjvcCtyas+UpV/aSq/gW4zkbsf05VnauqpapaWlhYuNeZJUlbGBL3y8DBJAeS7AKOA8tja74M/CZAkt1s3Ka5MctBJUnDbRn3qroDnAIuAdeAC1V1JcmZJEc3l10CfpzkKvAs8Imq+vH9GlqS9MqG3HOnqi4CF8fOPT3yuICPbf6RJM2Z71CVpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0NinuSI0muJ1lNcvoV1r0/SSVZmt2IkqRpbRn3JDuAs8CTwCHgRJJDE9a9Hvgo8I1ZDylJms6QnfthYLWqblTVbeA8cGzCuj8CPgX81wznkyTdgyFx3wPcHDle2zz3M0meAPZV1Vdf6YmSnEyykmRlfX196mElScMMiXsmnKufXUweAz4DfHyrJ6qqc1W1VFVLCwsLw6eUJE1lSNzXgH0jx3uBWyPHrwfeDnw9yfeAXweWfVFVkuZnSNwvAweTHEiyCzgOLP/fxap6qap2V9ViVS0CzwFHq2rlvkwsSdrSlnGvqjvAKeAScA24UFVXkpxJcvR+DyhJmt7OIYuq6iJwcezc03dZ+85XP5Yk6dXwHaqS1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqaFBcU9yJMn1JKtJTk+4/rEkV5O8kORrSd40+1ElSUNtGfckO4CzwJPAIeBEkkNjy74JLFXVrwJfAj4160ElScMN2bkfBlar6kZV3QbOA8dGF1TVs1X1n5uHzwF7ZzumJGkaQ+K+B7g5cry2ee5ungL+dtKFJCeTrCRZWV9fHz6lJGkqQ+KeCedq4sLkg8AS8OlJ16vqXFUtVdXSwsLC8CklSVPZOWDNGrBv5HgvcGt8UZJ3A38A/EZV/fdsxpMk3YshO/fLwMEkB5LsAo4Dy6MLkjwB/BlwtKp+OPsxJUnT2DLuVXUHOAVcAq4BF6rqSpIzSY5uLvs08Drgi0m+lWT5Lk8nSXoAhtyWoaouAhfHzj098vjdM55LkvQq+A5VSWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJamhnfMeQNI9eubxeU/QyzMvzXuCmXLnLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkOD4p7kSJLrSVaTnJ5w/TVJ/nrz+jeSLM56UEnScFvGPckO4CzwJHAIOJHk0Niyp4AXq+rNwGeAP5n1oJKk4Ybs3A8Dq1V1o6puA+eBY2NrjgF/ufn4S8C7kmR2Y0qSpjHkI3/3ADdHjteAX7vbmqq6k+Ql4I3Aj0YXJTkJnNw8/I8k1+9laE20m7F/74dR/JnuUbQtvjf55LbZj75pyKIhcZ/0N657WENVnQPODfiamlKSlapamvcc0ji/N+djyG2ZNWDfyPFe4Nbd1iTZCTwO/NssBpQkTW9I3C8DB5McSLILOA4sj61ZBn578/H7gb+rqpft3CVJD8aWt2U276GfAi4BO4DPVdWVJGeAlapaBj4LfD7JKhs79uP3c2hN5O0uPaz83pyDuMGWpH58h6okNWTcJakh4y5JDQ35PXc9ZJK8lY13Be9h4/0Et4Dlqro218EkPTTcuW8zSX6PjY+ACPDPbPyqaoAvTPpQN0mPJn9bZptJ8h3gV6rqJ2PndwFXqurgfCaTXlmSD1XVn897jkeFO/ft56fAL084/0ub16SH1SfnPcCjxHvu28/vAl9L8l3+/wPd9gNvBk7NbSoJSPLC3S4Bv/ggZ3nUeVtmG0ryGBsfxbyHjf80a8DlqvqfuQ6mR16SfwV+C3hx/BLwj1U16adO3Qfu3Lehqvop8Ny855Am+Crwuqr61viFJF9/8OM8uty5S1JDvqAqSQ0Zd0lqyLhLUkPGXZIa+l+hLyR0mB8liwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize = True).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_words(tweet):\n",
    "    alpha_only = re.sub(\"[^a-zA-Z]\",' ',tweet) #\"[^a-zA-Z]\" this regex will remove any non-alphabetical char as they are not significant\n",
    "    words = alpha_only.lower().split()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #from the dataframe we can see 'user' word is quite common in the tweets, which is basically used for tagging someone in the tweet\n",
    "    #so I will be removing that\n",
    "    stop.add('user')\n",
    "    sig_words = [word for word in words if not word in stop]\n",
    "    return(\" \".join(sig_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet']  = df['tweet'].apply(lambda tweet: clean_tweet_words(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('tweet',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>father disfunctional selfish drags kids disfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks lyft credit use cause offer wheelchair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>fucking weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>userthat shitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>huge fan fare big talking leave chaos pay disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>next school year year exams think school exams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>love land allin cavs champions cleveland cleve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>boring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                        clean_tweet\n",
       "0   1      1  father disfunctional selfish drags kids disfun...\n",
       "1   2      0  thanks lyft credit use cause offer wheelchair ...\n",
       "2   3      1                                      fucking weird\n",
       "3   4      1                                    userthat shitty\n",
       "4   5      0                      factsguide society motivation\n",
       "5   6      0  huge fan fare big talking leave chaos pay disp...\n",
       "6   7      0                             camping tomorrow danny\n",
       "7   8      0  next school year year exams think school exams...\n",
       "8   9      0  love land allin cavs champions cleveland cleve...\n",
       "9  10      1                                             boring"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "love     2828\n",
       "day      2393\n",
       "amp      1777\n",
       "happy    1707\n",
       "u        1193\n",
       "like     1180\n",
       "life     1176\n",
       "time     1149\n",
       "today    1095\n",
       "new      1003\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['clean_tweet']).split()).value_counts()[:10]  ##couting common word\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father disfunctional selfish drags kids disfun...\n",
       "1    thanks lyft credit use cause offer wheelchair ...\n",
       "2                                        fucking weird\n",
       "3                                      userthat shitty\n",
       "4                        factsguide society motivation\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq)) ##common word removal\n",
    "df['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thankful    952\n",
       "positive    937\n",
       "get         923\n",
       "people      885\n",
       "good        874\n",
       "bihday      873\n",
       "one         798\n",
       "see         764\n",
       "smile       747\n",
       "go          668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['clean_tweet']).split()).value_counts()[:10]  ##couting common word\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ernest               1\n",
       "datsyuk              1\n",
       "womensissues         1\n",
       "brecon               1\n",
       "harleythecockatoo    1\n",
       "medication           1\n",
       "memoire              1\n",
       "newlywedprobz        1\n",
       "stuggling            1\n",
       "mashup               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['clean_tweet']).split()).value_counts()[-10:] ##finding rare words\n",
    "freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father disfunctional selfish drags kids disfun...\n",
       "1    thanks lyft credit use cause offer wheelchair ...\n",
       "2                                        fucking weird\n",
       "3                                      userthat shitty\n",
       "4                        factsguide society motivation\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))  ## rare words removal\n",
    "df['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "\n",
    "#df['clean_tweet']=df['clean_tweet'][:3000].apply(lambda x: str(TextBlob(x).correct())) ## spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(axis=0, inplace=True)\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, disfunctional, selfish, drags, kids, ...\n",
       "1    [thanks, lyft, credit, use, cause, offer, whee...\n",
       "2                                     [fucking, weird]\n",
       "3                                   [userthat, shitty]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets=df['clean_tweet'].apply(lambda x:x.split())\n",
    "tokenized_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets=tokenized_tweets.apply(lambda x:[stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, disfunct, selfish, drag, kid, disfunc...\n",
       "1    [thank, lyft, credit, use, caus, offer, wheelc...\n",
       "2                                        [fuck, weird]\n",
       "3                                   [userthat, shitti]\n",
       "4                          [factsguid, societi, motiv]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweets)):\n",
    "    tokenized_tweets[i]=' '.join(tokenized_tweets[i])\n",
    "\n",
    "df['clean_tweet']=tokenized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "Y=df['label']\n",
    "X=df['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15981,), (15981,), (15981,), (15981,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.5,random_state=0)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([('tfidf',TfidfVectorizer()),('svc', LinearSVC(random_state=0,max_iter=5000))])\n",
    "nb_pipe = Pipeline([('tfidf',TfidfVectorizer()),('nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.fit(X_train,Y_train)\n",
    "nb_pipe.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_train)\n",
    "pred_nb = nb_pipe.predict(X_train)\n",
    "#pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "0.9948063325198674\n",
      "\n",
      "\n",
      "[[14823     6]\n",
      " [   77  1075]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_train,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "0.941305300043802\n",
      "\n",
      "\n",
      "[[14829     0]\n",
      " [  938   214]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_train,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.fit(X_test,Y_test)\n",
    "nb_pipe.fit(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_test)\n",
    "pred_nb = nb_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "0.9945560352919092\n",
      "\n",
      "\n",
      "[[14877     7]\n",
      " [   80  1017]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_test,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "0.9421187660346662\n",
      "\n",
      "\n",
      "[[14884     0]\n",
      " [  925   172]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_test,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
