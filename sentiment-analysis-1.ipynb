{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@user when a father is disfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>that was fucking hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>that was so shitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams. ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>you are so boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>#ireland consumer price index (mom) climbed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>we are so selfish. #orlando #standwithorlando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>i get to see my daddy today!!   #80days #getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>i won't spare you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>it was not good for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>ouch...junior is angry#got7 #junior #yugyoem  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for having a paner. #thankful #p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>its #friday! ð smiles all around via ig use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>as we all know, essential oils are not made of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>#euro2016 people blaming ha for conceded goal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>sad little dude..   #badday #coneofshame #cats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>product of the day: happy man #wine tool  who'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>@user #tgif   #ff to my #gamedev #indiedev #i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sign by vendor 80 for $45.00!! #upsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>@user all #smiles when #media is   !!  #press...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>we had a great panel on the mediatization of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>happy father's day @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>he said that i won't love you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                              tweet\n",
       "0    1      1   @user when a father is disfunctional and is s...\n",
       "1    2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2    3      1                               that was fucking hot\n",
       "3    4      1                               that was so shitty  \n",
       "4    5      0             factsguide: society now    #motivation\n",
       "5    6      0  [2/2] huge fan fare and big talking before the...\n",
       "6    7      0   @user camping tomorrow @user @user @user @use...\n",
       "7    8      0  the next school year is the year for exams. ca...\n",
       "8    9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9   10      1                                  you are so boring\n",
       "10  11      0    #ireland consumer price index (mom) climbed ...\n",
       "11  12      0  we are so selfish. #orlando #standwithorlando ...\n",
       "12  13      0  i get to see my daddy today!!   #80days #getti...\n",
       "13  14      1                               i won't spare you!  \n",
       "14  15      1                             it was not good for me\n",
       "15  16      1  ouch...junior is angry#got7 #junior #yugyoem  ...\n",
       "16  17      0  i am thankful for having a paner. #thankful #p...\n",
       "17  18      1                             retweet if you agree! \n",
       "18  19      0  its #friday! ð smiles all around via ig use...\n",
       "19  20      0  as we all know, essential oils are not made of...\n",
       "20  21      0  #euro2016 people blaming ha for conceded goal ...\n",
       "21  22      0  sad little dude..   #badday #coneofshame #cats...\n",
       "22  23      0  product of the day: happy man #wine tool  who'...\n",
       "23  24      1    @user @user lumpy says i am a . prove it lumpy.\n",
       "24  25      0   @user #tgif   #ff to my #gamedev #indiedev #i...\n",
       "25  26      0  beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26  27      0   @user all #smiles when #media is   !!  #press...\n",
       "27  28      0  we had a great panel on the mediatization of t...\n",
       "28  29      0                         happy father's day @user  \n",
       "29  30      1                      he said that i won't love you"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = df[df['label'] == 0]\n",
    "train_neg = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b476cce438>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize = True).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_words(tweet):\n",
    "    alpha_only = re.sub(\"[^a-zA-Z]\",' ',tweet) #\"[^a-zA-Z]\" this regex will remove any non-alphabetical char as they are not significant\n",
    "    words = alpha_only.lower().split()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #from the dataframe we can see 'user' word is quite common in the tweets, which is basically used for tagging someone in the tweet\n",
    "    #so I will be removing that\n",
    "    stop.add('user')\n",
    "    sig_words = [word for word in words if not word in stop]\n",
    "    return(\" \".join(sig_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet']  = df['tweet'].apply(lambda tweet: clean_tweet_words(tweet))\n",
    "\n",
    "df.drop('tweet',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>father disfunctional selfish drags kids disfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks lyft credit use cause offer wheelchair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>fucking hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>shitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>huge fan fare big talking leave chaos pay disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>next school year year exams think school exams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>love land allin cavs champions cleveland cleve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland consumer price index mom climbed previ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>selfish orlando standwithorlando pulseshooting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>get see daddy today days gettingfed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>spare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>ouch junior angry got junior yugyoem omg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>thankful paner thankful positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>friday smiles around via ig cookies make people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>know essential oils made chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>euro people blaming ha conceded goal fat roone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>sad little dude badday coneofshame cats pissed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>product day happy man wine tool weekend time o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>lumpy says prove lumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>tgif ff gamedev indiedev indiegamedev squad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sign vendor upsideofflorida shopalys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>smiles media pressconference antalya turkey su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>great panel mediatization public service ica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>happy father day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>said love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>luv hottweets like venusexchange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>new brochures arrived exciting aworks solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>much stuff happening florida first orlando sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>ferrari sake championship gp clearly turning p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>aced first test proud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>seeks probe udtapunjab leak points finger amar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>wrapping senseaboutmaths th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>hey white people call people white race identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>might shown today regurgitated talking points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>sometimes raise brows raise bar golfstrengthan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>greathonour careerconvos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>designing innovative learning space include wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>altright uses amp insecurity lure men whitesup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>carrying gun helped take gun control stop blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>use power mind heal body altwaystoheal healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>woohoo weeks go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>far away place family members hus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>ready rehearse tonight new music new videos lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>monday nights pm channel finally get see fuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>watching new episodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>offline nice long night snapchat redhead vermi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>things incredibly people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>yes received acceptance letter masters back oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>daughter riding bike around driveway son playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>omg loving station way jam work getting work d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>always hope one day get hug think gonna happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>couple sex fat naked japanese girls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>hump hump day humpers edwardsville pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>personalised gbp get shop cool home fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  label                                        clean_tweet\n",
       "0     1      1  father disfunctional selfish drags kids disfun...\n",
       "1     2      0  thanks lyft credit use cause offer wheelchair ...\n",
       "2     3      1                                        fucking hot\n",
       "3     4      1                                             shitty\n",
       "4     5      0                      factsguide society motivation\n",
       "5     6      0  huge fan fare big talking leave chaos pay disp...\n",
       "6     7      0                             camping tomorrow danny\n",
       "7     8      0  next school year year exams think school exams...\n",
       "8     9      0  love land allin cavs champions cleveland cleve...\n",
       "9    10      1                                             boring\n",
       "10   11      0  ireland consumer price index mom climbed previ...\n",
       "11   12      0  selfish orlando standwithorlando pulseshooting...\n",
       "12   13      0                get see daddy today days gettingfed\n",
       "13   14      1                                              spare\n",
       "14   15      1                                               good\n",
       "15   16      1           ouch junior angry got junior yugyoem omg\n",
       "16   17      0                   thankful paner thankful positive\n",
       "17   18      1                                      retweet agree\n",
       "18   19      0    friday smiles around via ig cookies make people\n",
       "19   20      0                 know essential oils made chemicals\n",
       "20   21      0  euro people blaming ha conceded goal fat roone...\n",
       "21   22      0  sad little dude badday coneofshame cats pissed...\n",
       "22   23      0  product day happy man wine tool weekend time o...\n",
       "23   24      1                             lumpy says prove lumpy\n",
       "24   25      0        tgif ff gamedev indiedev indiegamedev squad\n",
       "25   26      0  beautiful sign vendor upsideofflorida shopalys...\n",
       "26   27      0  smiles media pressconference antalya turkey su...\n",
       "27   28      0       great panel mediatization public service ica\n",
       "28   29      0                                   happy father day\n",
       "29   30      1                                          said love\n",
       "..  ...    ...                                                ...\n",
       "70   71      0                   luv hottweets like venusexchange\n",
       "71   72      0    new brochures arrived exciting aworks solutions\n",
       "72   73      0  much stuff happening florida first orlando sho...\n",
       "73   74      0  ferrari sake championship gp clearly turning p...\n",
       "74   75      0                              aced first test proud\n",
       "75   76      1  seeks probe udtapunjab leak points finger amar...\n",
       "76   77      0                        wrapping senseaboutmaths th\n",
       "77   78      1  hey white people call people white race identi...\n",
       "78   79      0  might shown today regurgitated talking points ...\n",
       "79   80      0  sometimes raise brows raise bar golfstrengthan...\n",
       "80   81      0                           greathonour careerconvos\n",
       "81   82      0  designing innovative learning space include wa...\n",
       "82   83      1  altright uses amp insecurity lure men whitesup...\n",
       "83   84      0  carrying gun helped take gun control stop blac...\n",
       "84   85      0  use power mind heal body altwaystoheal healthy...\n",
       "85   86      0                                    woohoo weeks go\n",
       "86   87      0                  far away place family members hus\n",
       "87   88      0  ready rehearse tonight new music new videos lo...\n",
       "88   89      0      monday nights pm channel finally get see fuss\n",
       "89   90      0                              watching new episodes\n",
       "90   91      0  offline nice long night snapchat redhead vermi...\n",
       "91   92      0                           things incredibly people\n",
       "92   93      0  yes received acceptance letter masters back oc...\n",
       "93   94      0  daughter riding bike around driveway son playi...\n",
       "94   95      0  omg loving station way jam work getting work d...\n",
       "95   96      0  always hope one day get hug think gonna happen...\n",
       "96   97      0                        model love u take u time ur\n",
       "97   98      0                couple sex fat naked japanese girls\n",
       "98   99      0    hump hump day humpers edwardsville pennsylvania\n",
       "99  100      0            personalised gbp get shop cool home fun\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, disfunctional, selfish, drags, kids, ...\n",
       "1    [thanks, lyft, credit, use, cause, offer, whee...\n",
       "2                                       [fucking, hot]\n",
       "3                                             [shitty]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "love     2828\n",
       "day      2393\n",
       "amp      1777\n",
       "happy    1707\n",
       "u        1193\n",
       "like     1180\n",
       "life     1176\n",
       "time     1149\n",
       "today    1095\n",
       "new      1003\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['clean_tweet']).split()).value_counts()[:10]  ##couting common word\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15981,), (15981,), (15981,), (15981,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.5,random_state=0)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([('tfidf',TfidfVectorizer()),('svc', LinearSVC(random_state=0,max_iter=5000))])\n",
    "nb_pipe = Pipeline([('tfidf',TfidfVectorizer()),('nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe.fit(X_train,Y_train)\n",
    "nb_pipe.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_train)\n",
    "pred_nb = nb_pipe.predict(X_train)\n",
    "#pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_train,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_train,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe.fit(X_test,Y_test)\n",
    "nb_pipe.fit(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_test)\n",
    "pred_nb = nb_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_test,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_test,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, disfunctional, selfish, drags, kids, ...\n",
       "1    [thanks, lyft, credit, use, cause, offer, whee...\n",
       "2                                       [fucking, hot]\n",
       "3                                             [shitty]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet=tokenized_tweet.apply(lambda x:[stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, disfunct, selfish, drag, kid, disfunc...\n",
       "1    [thank, lyft, credit, use, caus, offer, wheelc...\n",
       "2                                          [fuck, hot]\n",
       "3                                             [shitti]\n",
       "4                          [factsguid, societi, motiv]\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i]=' '.join(tokenized_tweet[i])\n",
    "\n",
    "df['clean_tweet']=tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "Y=df['label']\n",
    "X=df['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22373,), (9589,), (22373,), (9589,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=0)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([('tfidf',TfidfVectorizer()),('svc', LinearSVC(random_state=0,max_iter=5000))])\n",
    "nb_pipe = Pipeline([('tfidf',TfidfVectorizer()),('nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.fit(X_train,Y_train)\n",
    "nb_pipe.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_train)\n",
    "pred_nb = nb_pipe.predict(X_train)\n",
    "#pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "0.994055334555044\n",
      "\n",
      "\n",
      "[[20748    13]\n",
      " [  120  1492]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_train,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "0.9430116658472266\n",
      "\n",
      "\n",
      "[[20761     0]\n",
      " [ 1275   337]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_train,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_train,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.fit(X_test,Y_test)\n",
    "nb_pipe.fit(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc_pipe.predict(X_test)\n",
    "pred_nb = nb_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "0.9970799874856606\n",
      "\n",
      "\n",
      "[[8950    2]\n",
      " [  26  611]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVC')\n",
    "print(accuracy_score(Y_test,pred_svc))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_svc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "0.9402440296172698\n",
      "\n",
      "\n",
      "[[8952    0]\n",
      " [ 573   64]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Classifier')\n",
    "print(accuracy_score(Y_test,pred_nb))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test,pred_nb))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
